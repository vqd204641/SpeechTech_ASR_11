{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11800680,"sourceType":"datasetVersion","datasetId":7410707},{"sourceId":11800853,"sourceType":"datasetVersion","datasetId":7410832},{"sourceId":11845255,"sourceType":"datasetVersion","datasetId":7442431},{"sourceId":11845849,"sourceType":"datasetVersion","datasetId":7442849},{"sourceId":399977,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":327315,"modelId":348197}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Setup**","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers torchaudio accelerate librosa evaluate jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:02:07.518748Z","iopub.execute_input":"2025-05-18T02:02:07.519153Z","iopub.status.idle":"2025-05-18T02:03:23.666962Z","shell.execute_reply.started":"2025-05-18T02:02:07.519128Z","shell.execute_reply":"2025-05-18T02:03:23.666231Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Load data**","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\ndef create_manifest(folder_path, output_jsonl, max_samples=None):\n    samples = []\n    for filename in os.listdir(folder_path):\n        if filename.endswith(\".wav\"):\n            base_name = filename[:-4]\n            wav_path = os.path.join(folder_path, filename)\n            txt_path = os.path.join(folder_path, base_name + \".txt\")\n            if os.path.exists(txt_path):\n                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n                    transcription = f.read().strip()\n                samples.append({\n                    \"audio\": wav_path,\n                    \"transcription\": transcription\n                })\n            if max_samples and len(samples) >= max_samples:\n                break\n\n    with open(output_jsonl, \"w\", encoding=\"utf-8\") as f:\n        for sample in samples:\n            json.dump(sample, f, ensure_ascii=False)\n            f.write(\"\\n\")\n\ncreate_manifest(\"/kaggle/input/train1-asr/audio_short\", \"train.jsonl\")\ncreate_manifest(\"/kaggle/input/test-asr/test\", \"test.jsonl\", max_samples=1000)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:02:05.115492Z","iopub.execute_input":"2025-05-18T02:02:05.115820Z","iopub.status.idle":"2025-05-18T02:02:05.451967Z","shell.execute_reply.started":"2025-05-18T02:02:05.115791Z","shell.execute_reply":"2025-05-18T02:02:05.451318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, Audio\n\ntrain_data = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\ntest_data = load_dataset(\"json\", data_files=\"test.jsonl\", split=\"train\")\n\ntrain_data = train_data.cast_column(\"audio\", Audio())\ntest_data = test_data.cast_column(\"audio\", Audio())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:02:05.453324Z","iopub.execute_input":"2025-05-18T02:02:05.453660Z","iopub.status.idle":"2025-05-18T02:02:07.517877Z","shell.execute_reply.started":"2025-05-18T02:02:05.453640Z","shell.execute_reply":"2025-05-18T02:02:07.517233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = train_data.map(prepare_dataset)\ntest_data = test_data.map(prepare_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:03:51.588039Z","iopub.execute_input":"2025-05-18T02:03:51.588654Z","iopub.status.idle":"2025-05-18T02:04:06.296843Z","shell.execute_reply.started":"2025-05-18T02:03:51.588632Z","shell.execute_reply":"2025-05-18T02:04:06.296226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Setup model/load model** ","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperTokenizer, WhisperProcessor, WhisperFeatureExtractor, WhisperForConditionalGeneration, TrainingArguments, Seq2SeqTrainer\nimport torch\n\n# model_name = \"openai/whisper-small\"\n\n# processor = WhisperProcessor.from_pretrained(model_name)\n# tokenizer = processor.tokenizer\n# feature_extractor = processor.feature_extractor\n# model = WhisperForConditionalGeneration.from_pretrained(model_name)\nmodel_path = \"/kaggle/input/train1_asr/transformers/default/1/checkpoint-419\"\n\n# Load processor và model từ thư mục local\nprocessor = WhisperProcessor.from_pretrained(model_path)\ntokenizer = processor.tokenizer\nfeature_extractor = processor.feature_extractor\nmodel = WhisperForConditionalGeneration.from_pretrained(model_path)\n# Xử lý dữ liệu đầu vào\ndef prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n    # Lấy đặc trưng đầu vào\n    inputs = processor.feature_extractor(audio[\"array\"], sampling_rate=16000)\n    batch[\"input_features\"] = inputs[\"input_features\"][0]\n\n    # Encode target text\n    batch[\"labels\"] = processor.tokenizer(\n        batch[\"transcription\"],\n        padding=\"max_length\",\n        max_length=128,\n        truncation=True\n    ).input_ids\n\n    return batch\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:03:23.668826Z","iopub.execute_input":"2025-05-18T02:03:23.669044Z","iopub.status.idle":"2025-05-18T02:03:51.587197Z","shell.execute_reply.started":"2025-05-18T02:03:23.669024Z","shell.execute_reply":"2025-05-18T02:03:51.586412Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\nwer_metric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    if isinstance(pred_ids, tuple):  # Nếu là logits\n        pred_ids = np.argmax(pred_ids[0], axis=-1)\n\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n\n    pred_str = processor.tokenizer.batch_decode(pred_ids.tolist(), skip_special_tokens=True)\n    label_str = processor.tokenizer.batch_decode(label_ids.tolist(), skip_special_tokens=True)\n\n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n    return {\"wer\": wer}\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:04:06.297648Z","iopub.execute_input":"2025-05-18T02:04:06.297888Z","iopub.status.idle":"2025-05-18T02:04:07.052224Z","shell.execute_reply.started":"2025-05-18T02:04:06.297869Z","shell.execute_reply":"2025-05-18T02:04:07.051731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n# Định nghĩa training args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-finetune1\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=1,\n    fp16=True,  \n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    learning_rate=1e-5,\n    warmup_steps=500,\n    save_total_limit=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    disable_tqdm=False ,\n    report_to=[],  \n)\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    tokenizer =processor.feature_extractor,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:04:07.052914Z","iopub.execute_input":"2025-05-18T02:04:07.053150Z","iopub.status.idle":"2025-05-18T02:04:07.056913Z","shell.execute_reply.started":"2025-05-18T02:04:07.053132Z","shell.execute_reply":"2025-05-18T02:04:07.056210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:04:07.057802Z","iopub.execute_input":"2025-05-18T02:04:07.058157Z","iopub.status.idle":"2025-05-18T02:04:07.073762Z","shell.execute_reply.started":"2025-05-18T02:04:07.058135Z","shell.execute_reply":"2025-05-18T02:04:07.073045Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Train**","metadata":{}},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ncreate_manifest(\"/kaggle/input/tran2-asr/audio_short\", \"train.jsonl\")\ntrain_data1 = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\ntrain_data1 = train_data1.cast_column(\"audio\", Audio())\ntrain_data1 = train_data1.map(prepare_dataset)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-finetune1\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=2,\n    fp16=True,  \n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    learning_rate=1e-5,\n    warmup_steps=500,\n    save_total_limit=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    disable_tqdm=False ,\n    report_to=[],  \n)\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=train_data1,\n    eval_dataset=test_data,\n    tokenizer =processor.feature_extractor,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:04:38.312028Z","iopub.execute_input":"2025-05-18T02:04:38.312630Z","iopub.status.idle":"2025-05-18T02:05:06.740650Z","shell.execute_reply.started":"2025-05-18T02:04:38.312605Z","shell.execute_reply":"2025-05-18T02:05:06.739906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"create_manifest(\"/kaggle/input/train3-asr/data_fillter1/audio_short\", \"train.jsonl\")\ntrain_data2 = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\ntrain_data2 = train_data2.cast_column(\"audio\", Audio())\ntrain_data2 = train_data2.map(prepare_dataset)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-finetune2\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=5,\n    fp16=True,  \n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    learning_rate=1e-5,\n    warmup_steps=500,\n    save_total_limit=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    disable_tqdm=False ,\n    report_to=[],  \n)\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=train_data2,\n    eval_dataset=test_data,\n    tokenizer =processor.feature_extractor,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:05:14.667975Z","iopub.execute_input":"2025-05-18T02:05:14.668587Z","iopub.status.idle":"2025-05-18T02:05:35.575943Z","shell.execute_reply.started":"2025-05-18T02:05:14.668561Z","shell.execute_reply":"2025-05-18T02:05:35.575103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Gen transcripts**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nfrom transformers import GenerationConfig\nfrom transformers import WhisperForConditionalGeneration, AutoProcessor\n\ndef predict_audio(audio_path, model, processor, sampling_rate=16000, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    waveform, sr = torchaudio.load(audio_path)\n\n    if sr != sampling_rate:\n        resampler = torchaudio.transforms.Resample(sr, sampling_rate)\n        waveform = resampler(waveform)\n\n    if waveform.shape[0] > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)\n\n    input_features = processor.feature_extractor(\n        waveform.squeeze().numpy(),\n        sampling_rate=sampling_rate,\n        return_tensors=\"pt\"\n    ).input_features.to(device)\n\n    model = model.to(device)\n\n    if hasattr(model.config, 'forced_decoder_ids'):\n        model.config.forced_decoder_ids = None\n    if hasattr(model, 'generation_config') and hasattr(model.generation_config, 'forced_decoder_ids'):\n        model.generation_config.forced_decoder_ids = None\n\n    model.generation_config = GenerationConfig(\n        max_length=128,\n        pad_token_id=processor.tokenizer.pad_token_id,\n        eos_token_id=processor.tokenizer.eos_token_id,\n        decoder_start_token_id=model.config.decoder_start_token_id,\n        use_cache=False\n    )\n\n    with torch.no_grad():\n        predicted_ids = model.generate(input_features)\n\n    transcription = processor.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n\n    return transcription\n\nif __name__ == \"__main__\":\n    model_name = \"/kaggle/input/asr_final/transformers/default/1\"\n    processor = AutoProcessor.from_pretrained(model_name)\n    model = WhisperForConditionalGeneration.from_pretrained(model_name)\n\n    input_dir = \"/kaggle/input/asr-data/private_test/private-test-data-asr\"\n    output_file = \"/kaggle/working/predictions1.txt\"\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n        for filename in sorted(os.listdir(input_dir)):\n            if filename.endswith((\".wav\", \".mp3\")):\n                file_path = os.path.join(input_dir, filename)\n\n                # Nếu là .mp3 thì convert sang .wav tạm thời\n                if filename.endswith(\".mp3\"):\n                    tmp_path = \"/kaggle/working/temp.wav\"\n                    try:\n                        waveform, sr = torchaudio.load(file_path)\n                        torchaudio.save(tmp_path, waveform, sr)\n                        audio_path = tmp_path\n                    except Exception as e:\n                        print(f\"✗ Error converting {filename}: {e}\")\n                        continue\n                else:\n                    audio_path = file_path\n\n                try:\n                    transcription = predict_audio(audio_path, model, processor)\n                    audio_id = os.path.splitext(filename)[0]\n                    out_f.write(f\"{audio_id} {transcription.strip()}\\n\")\n                    print(f\"✓ {audio_id}\")\n                except Exception as e:\n                    print(f\"✗ Error processing {filename}: {e}\")\n\n        temp_path = \"/kaggle/working/temp.wav\"\n        if os.path.exists(temp_path):\n            os.remove(temp_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Sử dụng LLM** ","metadata":{}},{"cell_type":"code","source":"\n!pip install requests\n!pip install google-generativeai\n \nimport requests\nimport json\n \nAPI_KEY = \"AIzaSyAPqOHiASbvA8PKRYS8QhSnfK_idUNu0Ww\"\nURL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={API_KEY}\"\n \ndef get_gemini_response(question):\n    headers = {\n        \"Content-Type\": \"application/json\"\n    }\n \n    data = {\n        \"contents\": [\n            {\n                \"parts\": [\n                    {\"text\": question}\n                ]\n            }\n        ]\n    }\n \n    response = requests.post(URL, headers=headers, json=data)\n \n    if response.status_code == 200:\n        result = response.json()\n        answer = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n        return answer\n    else:\n        return f\"Lỗi: {response.status_code} - {response.text}\"\n \ncau_ban_dau = \"Hôm qua tui đi New York chơi, gặp thằng bạn từ Washington qua, nó rủ đi ăn burger ở McDonald's\"\n \nprompt = f\"\"\"Viết lại câu cho tôi loại bỏ những lỗi chính tả.\n              Câu ban đầu là: {cau_ban_dau}.viết lại cho tôi theo yêu cầu trên\"\"\"\n \nanswer = get_gemini_response(prompt)\nprint(\"Câu trả lời:\", answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:06:44.157417Z","iopub.execute_input":"2025-05-18T02:06:44.157981Z","iopub.status.idle":"2025-05-18T02:06:45.304306Z","shell.execute_reply.started":"2025-05-18T02:06:44.157957Z","shell.execute_reply":"2025-05-18T02:06:45.303720Z"}},"outputs":[],"execution_count":null}]}